{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.9 64-bit ('mlenv': conda)",
   "metadata": {
    "interpreter": {
     "hash": "71d2ad3b2737414c0184e52001d3bfd669dd7705f0b48cb8b421db1e3c007459"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from config import db_password\n",
    "\n",
    "import time"
   ]
  },
  {
   "source": [
    "# Sample Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DF\n",
    "sample_df = pd.read_csv(\"Resources/sample.csv\",encoding='ISO-8859-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sample_id                 int64\n",
       "sample_name              object\n",
       "latitude                float64\n",
       "longitude               float64\n",
       "loc_prec                float64\n",
       "qgis_geom                object\n",
       "datum                    object\n",
       "depth                   float64\n",
       "material                 object\n",
       "rock_name                object\n",
       "protolith                object\n",
       "sample_description       object\n",
       "density                 float64\n",
       "comments                 object\n",
       "qap_name                 object\n",
       "sia_scheme               object\n",
       "frost_class1             object\n",
       "frost_class2             object\n",
       "frost_class3             object\n",
       "quartz                  float64\n",
       "feldspar                float64\n",
       "lithics                 float64\n",
       "facies                   object\n",
       "texture                  object\n",
       "p_velocity              float64\n",
       "density_model           float64\n",
       "heat_production         float64\n",
       "heat_production_mass    float64\n",
       "ref_id                    int64\n",
       "iso_id                    int64\n",
       "comp_id                   int64\n",
       "major_id                  int64\n",
       "trace_id                  int64\n",
       "rgroup_id                 int64\n",
       "age_id                    int64\n",
       "method_id                 int64\n",
       "country_id                int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# View data types\n",
    "sample_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns that aren't useful\n",
    "clean_sample_df = sample_df.drop(['loc_prec','qgis_geom','datum','depth','material','protolith','sample_description','density','comments','qap_name','sia_scheme','frost_class1','frost_class2','frost_class3','facies','texture','ref_id','method_id'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "sample_id                 int64\n",
       "sample_name              object\n",
       "latitude                float64\n",
       "longitude               float64\n",
       "rock_name                object\n",
       "quartz                  float64\n",
       "feldspar                float64\n",
       "lithics                 float64\n",
       "p_velocity              float64\n",
       "density_model           float64\n",
       "heat_production         float64\n",
       "heat_production_mass    float64\n",
       "iso_id                    int64\n",
       "comp_id                   int64\n",
       "major_id                  int64\n",
       "trace_id                  int64\n",
       "rgroup_id                 int64\n",
       "age_id                    int64\n",
       "country_id                int64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "# View data types in clean_df\n",
    "clean_sample_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database string\n",
    "db_string = f\"postgres://postgres:{db_password}@127.0.0.1:5432/whole_rock_comp\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DB Engine\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df to clean_csv\n",
    "clean_sample_df.to_csv('Resources/clean_sample.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing rows 0 to 100000...Done.\n",
      "importing rows 100000 to 200000...Done.\n",
      "importing rows 200000 to 300000...Done.\n",
      "importing rows 300000 to 400000...Done.\n",
      "importing rows 400000 to 500000...Done.\n",
      "importing rows 500000 to 600000...Done.\n",
      "importing rows 600000 to 700000...Done.\n",
      "importing rows 700000 to 800000...Done.\n",
      "importing rows 800000 to 900000...Done.\n",
      "importing rows 900000 to 1000000...Done.\n",
      "importing rows 1000000 to 1022092...Done.\n"
     ]
    }
   ],
   "source": [
    "# Load clean_csv to PostgreSQL DB\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv('Resources/clean_sample.csv', chunksize=100000):\n",
    "\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='Sample', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "source": [
    "# Trace Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DF\n",
    "trace_df = pd.read_csv(\"Resources/trace.csv\",encoding='ISO-8859-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "trace_id      int64\n",
       "f_ppm       float64\n",
       "cl_ppm      float64\n",
       "br_ppm      float64\n",
       "i_ppm       float64\n",
       "h_ppm       float64\n",
       "c_ppm       float64\n",
       "n_ppm       float64\n",
       "p_ppm       float64\n",
       "s_ppm       float64\n",
       "al_ppm      float64\n",
       "as_ppm      float64\n",
       "ag_ppm      float64\n",
       "au_ppm      float64\n",
       "b_ppm       float64\n",
       "ba_ppm      float64\n",
       "be_ppm      float64\n",
       "bi_ppm      float64\n",
       "ca_ppm      float64\n",
       "cd_ppm      float64\n",
       "ce_ppm      float64\n",
       "co_ppm      float64\n",
       "cr_ppm      float64\n",
       "cs_ppm      float64\n",
       "cu_ppm      float64\n",
       "dy_ppm      float64\n",
       "er_ppm      float64\n",
       "eu_ppm      float64\n",
       "fe_ppm      float64\n",
       "ga_ppm      float64\n",
       "gd_ppm      float64\n",
       "ge_ppm      float64\n",
       "hf_ppm      float64\n",
       "hg_ppm      float64\n",
       "ho_ppm      float64\n",
       "in_ppm      float64\n",
       "ir_ppm      float64\n",
       "k_ppm       float64\n",
       "la_ppm      float64\n",
       "li_ppm      float64\n",
       "lu_ppm      float64\n",
       "mg_ppm      float64\n",
       "mn_ppm      float64\n",
       "mo_ppm      float64\n",
       "na_ppm      float64\n",
       "nd_ppm      float64\n",
       "ni_ppm      float64\n",
       "nb_ppm      float64\n",
       "os_ppm      float64\n",
       "pa_ppm      float64\n",
       "pb_ppm      float64\n",
       "pd_ppm      float64\n",
       "pm_ppm      float64\n",
       "pr_ppm      float64\n",
       "pt_ppm      float64\n",
       "rb_ppm      float64\n",
       "re_ppm      float64\n",
       "rh_ppm      float64\n",
       "ru_ppm      float64\n",
       "sb_ppm      float64\n",
       "sc_ppm      float64\n",
       "se_ppm      float64\n",
       "si_ppm      float64\n",
       "sm_ppm      float64\n",
       "sn_ppm      float64\n",
       "sr_ppm      float64\n",
       "ta_ppm      float64\n",
       "tb_ppm      float64\n",
       "te_ppm      float64\n",
       "th_ppm      float64\n",
       "ti_ppm      float64\n",
       "tl_ppm      float64\n",
       "tm_ppm      float64\n",
       "w_ppm       float64\n",
       "v_ppm       float64\n",
       "u_ppm       float64\n",
       "y_ppm       float64\n",
       "yb_ppm      float64\n",
       "zn_ppm      float64\n",
       "zr_ppm      float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "# View data types\n",
    "trace_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df to clean_csv\n",
    "trace_df.to_csv('Resources/clean_trace.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing rows 0 to 100000...Done.\n",
      "importing rows 100000 to 200000...Done.\n",
      "importing rows 200000 to 300000...Done.\n",
      "importing rows 300000 to 400000...Done.\n",
      "importing rows 400000 to 500000...Done.\n",
      "importing rows 500000 to 600000...Done.\n",
      "importing rows 600000 to 700000...Done.\n",
      "importing rows 700000 to 771361...Done.\n"
     ]
    }
   ],
   "source": [
    "# Load clean_csv to PostgreSQL DB\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv('Resources/clean_trace.csv', chunksize=100000):\n",
    "\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='Trace', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "source": [
    "# ISOTOPE TABLE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DF\n",
    "isotope_df = pd.read_csv(\"Resources/isotope.csv\",encoding='ISO-8859-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "iso_id           int64\n",
       "rb87_sr86      float64\n",
       "rb87_sr87      float64\n",
       "sr87_sr86      float64\n",
       "nd143_nd144    float64\n",
       "sm147_nd144    float64\n",
       "lu176_hf177    float64\n",
       "hf176_hf177    float64\n",
       "re187_os186    float64\n",
       "re187_os188    float64\n",
       "os187_os188    float64\n",
       "pb206_pb204    float64\n",
       "pb207_pb204    float64\n",
       "pb208_pb204    float64\n",
       "th232_pb204    float64\n",
       "th232_u238     float64\n",
       "u238_pb204     float64\n",
       "epsilon_hf     float64\n",
       "epsilon_nd     float64\n",
       "epsilon_sr     float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "# View data types\n",
    "isotope_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df to clean_csv\n",
    "isotope_df.to_csv('Resources/clean_isotope.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing rows 0 to 81475...Done.\n"
     ]
    }
   ],
   "source": [
    "# Load clean_csv to PostgreSQL DB\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv('Resources/clean_isotope.csv', chunksize=100000):\n",
    "\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='Isotope', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "source": [
    "# COMPUTED TABLE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DF\n",
    "computed_df = pd.read_csv(\"Resources/computed.csv\",encoding='ISO-8859-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "comp_id        int64\n",
       "mg_number    float64\n",
       "fe_number    float64\n",
       "mali         float64\n",
       "asi          float64\n",
       "maficity     float64\n",
       "cia          float64\n",
       "wip          float64\n",
       "spar         float64\n",
       "qtzindex     float64\n",
       "r1           float64\n",
       "r2           float64\n",
       "rock_type     object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "# View data types\n",
    "computed_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df to clean_csv\n",
    "computed_df.to_csv('Resources/clean_computed.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing rows 0 to 100000...Done.\n",
      "importing rows 100000 to 200000...Done.\n",
      "importing rows 200000 to 300000...Done.\n",
      "importing rows 300000 to 400000...Done.\n",
      "importing rows 400000 to 500000...Done.\n",
      "importing rows 500000 to 600000...Done.\n",
      "importing rows 600000 to 666104...Done.\n"
     ]
    }
   ],
   "source": [
    "# Load clean_csv to PostgreSQL DB\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv('Resources/clean_computed.csv', chunksize=100000):\n",
    "\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='Computed', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "source": [
    "# Major Table"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DF\n",
    "major_df = pd.read_csv(\"Resources/major.csv\",encoding='ISO-8859-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "major_id       int64\n",
       "sio2         float64\n",
       "tio2         float64\n",
       "al2o3        float64\n",
       "cr2o3        float64\n",
       "fe2o3        float64\n",
       "fe2o3_tot    float64\n",
       "feo          float64\n",
       "feo_tot      float64\n",
       "mgo          float64\n",
       "cao          float64\n",
       "mno          float64\n",
       "nio          float64\n",
       "k2o          float64\n",
       "na2o         float64\n",
       "sro          float64\n",
       "p2o5         float64\n",
       "h2o_plus     float64\n",
       "h2o_minus    float64\n",
       "h2o_tot      float64\n",
       "co2          float64\n",
       "so3          float64\n",
       "bao          float64\n",
       "caco3        float64\n",
       "mgco3        float64\n",
       "loi          float64\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 38
    }
   ],
   "source": [
    "# View data types\n",
    "major_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df to clean_csv\n",
    "major_df.to_csv('Resources/clean_major.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing rows 0 to 100000...Done.\n",
      "importing rows 100000 to 200000...Done.\n",
      "importing rows 200000 to 300000...Done.\n",
      "importing rows 300000 to 400000...Done.\n",
      "importing rows 400000 to 500000...Done.\n",
      "importing rows 500000 to 600000...Done.\n",
      "importing rows 600000 to 661994...Done.\n"
     ]
    }
   ],
   "source": [
    "# Load clean_csv to PostgreSQL DB\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv('Resources/clean_major.csv', chunksize=100000):\n",
    "\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='Major', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "source": [
    "# ROCKGROUP TABLE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DF\n",
    "rockgroup_df = pd.read_csv(\"Resources/rockgroup.csv\",encoding='ISO-8859-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "rgroup_id       int64\n",
       "rock_group     object\n",
       "rock_origin    object\n",
       "rock_facies    object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 45
    }
   ],
   "source": [
    "rockgroup_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df to clean_csv\n",
    "rockgroup_df.to_csv('Resources/clean_rockgroup.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing rows 0 to 91...Done.\n"
     ]
    }
   ],
   "source": [
    "# Load clean_csv to PostgreSQL DB\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv('Resources/clean_rockgroup.csv', chunksize=100000):\n",
    "\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='Rockgroup', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "source": [
    "# AGE TABLE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DF\n",
    "age_df = pd.read_csv(\"Resources/age.csv\",encoding='ISO-8859-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "age_id               int64\n",
       "age_min            float64\n",
       "age                float64\n",
       "age_max            float64\n",
       "age_sd             float64\n",
       "time_period_min     object\n",
       "time_period         object\n",
       "time_period_max     object\n",
       "age_method          object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "age_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean_df to clean_csv\n",
    "age_df.to_csv('Resources/clean_age.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing rows 0 to 43719...Done.\n"
     ]
    }
   ],
   "source": [
    "# Load clean_csv to PostgreSQL DB\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv('Resources/clean_age.csv', chunksize=100000):\n",
    "\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='Age', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "source": [
    "# COUNTRY TABLE"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV into DF\n",
    "country_df = pd.read_csv(\"Resources/country.csv\",encoding='ISO-8859-1' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "country_id     int64\n",
       "country       object\n",
       "dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 57
    }
   ],
   "source": [
    "country_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_df.to_csv('Resources/clean_country.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "importing rows 0 to 227...Done.\n"
     ]
    }
   ],
   "source": [
    "# Load clean_csv to PostgreSQL DB\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv('Resources/clean_country.csv', chunksize=100000):\n",
    "\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='Country', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    print(f'Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}